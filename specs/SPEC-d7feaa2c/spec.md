# SPEC-d7feaa2c: Nodeエンジンローダー抽象化とNemotron直接ロード

## 背景 / 問題
Nodeは現在 llama.cpp (GGUF) を前提にしており、モデル形式の多様化（safetensors等）に対応できない。
Nemotron 3 Nano 30B A3B(BF16) はGGUF変換が失敗するため、safetensorsを直接実行できる新エンジンが必要。
ただし、Nemotron向けの新エンジン（推論エンジン）の仕様と実装は後回しにし、後で決める（TBD）。

## 目的
本仕様は統合仕様 `SPEC-3fc2c1e4`（実行エンジン）の**詳細仕様**として扱う。

- Node側でエンジンローダーを抽象化し、複数エンジンを共存可能にする
- 内蔵エンジンは **プラグイン形式（動的ロード）** とし、後から同一ABIで追加可能にする
- エンジン選択は「登録時に選択したアーティファクト（safetensors/GGUF）」と
  Hugging Face の `config.json` 等のモデル由来メタデータを正として判定する

## ゴール
- Nodeにエンジン抽象化レイヤーが導入され、llama.cpp 等の複数エンジンを選択できる
- エンジンは **動的プラグイン** として追加・更新できる
- `metadata.json` のような llm-router 独自メタデータファイルに依存せず、エンジン選択が実装される
- **対応OS/GPUの前提が明確**（macOS: Metal、Windows: DirectML、Linux: 非対応/CUDAは実験）

## アーキテクチャ（Nodeエンジン層）

### 目的
- **モデル形式や実行方式が増えても**、Node側のモデル検出・ロード・推論の責務が破綻しない構造にする。
- **“正（登録で確定したアーティファクト）” と “実行（Engine）”** を分離し、拡張を容易にする。

### コンポーネント分解（概念）

```
Router（登録・配布）                Node（取得・検証・実行）
───────────────────               ─────────────────────────
register(format=...)  ───────▶    ModelStorage
  ├─ 形式確定                           ├─ ローカル配置検出
  ├─ 必須ファイル検証                    ├─ HF由来メタデータ検証
  └─ マニフェスト確定                    └─ ModelDescriptor生成
                                           ├─ 共有パス or 外部ソース/プロキシ取得
                                           └─ GPUバックエンドに応じた取得選択
                                           │
                                           ▼
                                    EngineRegistry
                                      └─ runtime → Engine を解決
                                           │
                                           ▼
                                   EngineHost (Plugin Loader)
                                     └─ Pluginへ推論を委譲
                                           │
                                           ▼
                                       Engine Plugins（複数）
                                     ├─ llama.cpp（GGUF）
                                     └─ safetensors系（将来拡張）
```

### Key concepts
- **ModelDescriptor**: Nodeが推論を開始するために必要な最小情報（例: `format`, `runtime`, `model_dir`, `primary` など）。
- **runtime**: “どのEngineで実行すべきか” を表す識別子（例: `llama_cpp` など）。
- **Engine**: 推論の実体（GPU実行・サンプリング・ストリーミングなど）を担う差し替え可能ユニット。
- **Engine Plugin**: 共有ライブラリ + manifest.json で提供される実行ユニット。
- **Engine Host**: プラグインの発見・ロード・ライフサイクル管理のみ担当するホスト層。

### エンジン選択の入力（Single source of truth）
- **登録時に確定したアーティファクト（format/必要ファイル）**
- **Hugging Face 由来のメタデータ（`config.json` 等）**

※ Nodeローカルに複数形式が同居していることを理由に自動フォールバックは行わない（登録で確定させる）。

### OS/GPU前提（本仕様の前提）
- macOS: Apple Silicon + Metal
- Windows: DirectML（D3D12）
- Linux: 当面は非対応（CUDAは実験扱い）

## 非ゴール
- Nemotron向けの新エンジン（推論エンジン）の仕様策定・実装（後回し / TBDとして別途扱う）
- Nemotron 推論の高速化・最適化
- Python依存の導入
- GGUF変換の改善
- プラグインサンドボックス/権限分離

## ユーザーストーリー
- 開発者として、モデル形式が増えてもNodeのエンジン選択が壊れない構造にしたい

## 保留事項（TBD）
- Nemotron向けの新エンジン（推論エンジン）は別SPECとして後日仕様化する（後回し / TBD。Metal/DirectML対応方針、CUDA実験扱い、chat_template互換、dtype戦略など）。

## 受け入れ条件
- Nodeが登録時の選択（safetensors/GGUF）と `config.json` に従ってエンジンを選択する
- `metadata.json` に依存しない
- エンジン判定結果は /v1/models の応答に影響し、未対応モデルは登録対象から除外できる
- Python依存は導入しない
- Engine は動的プラグインであり、ABIバージョン一致のもののみロードされる

---

## Clarifications

### Session 2025-12-30

インタビューにより以下が確定（詳細は統合仕様 `SPEC-3fc2c1e4` を参照）:

**プラグイン実装詳細**:

- **サードパーティプラグイン**: サポートする（制限なし）
  - ディレクトリ配置のみでインストール可能
- **ABI不一致時**: ロード拒否 + アラート（通知のみ、互換レイヤーなし）
- **ID競合時**: ロードエラー（2つ目以降は拒否）
- **ネットワークアクセス**: 禁止（エンジンからの外部通信は不可）

**モダリティ対応**:

- 全モダリティ（テキスト/画像/音声）で同一Engineインターフェースを統一
- 入出力はモダリティ別メソッド（generate_text(), generate_image()等）で分離

**エンジン選択とベンチマーク**:

- 同一モデルに複数エンジンが対応可能な場合、モデル登録時のベンチマーク結果で決定
- 複合スコア（スループット + TTFT + VRAM使用率）で評価
- ベンチマーク結果はモデルメタデータに埋め込み保存

### Session 2025-12-30 (詳細インタビュー)

エンジンプラグインの詳細仕様については、統合仕様 `SPEC-3fc2c1e4` の
「Session 2025-12-30 (詳細インタビュー)」セクションを参照。

本SPECに関連する主要決定事項:

**アーキテクチャ**:

- モノリシック構成（同一プロセス内でEngine Hostとプラグインが動作）
- ハング時はノード全体を再起動（プロセス分離は将来検討）
- ウォッチドッグスレッドで30秒タイムアウト監視

**エンジン選択とロード**:

- RuntimeType競合時はベンチマーク結果で自動選択
- 遅延ロード（初回リクエスト時にロード）
- LRU順でアンロード（VRAM 90%閾値）

**GPU対応**:

- GPU別に別バイナリ（engines/{engine_id}/{gpu_backend}/{binary}）
- manifest.jsonでGPUバックエンド指定

**エラー処理**:

- GPU競合時は即座に429エラー
- VRAM OOM時はエラー返却後、次リクエストは処理継続
- クライアント切断時は即座に処理中断

### Session 2025-12-31 (内蔵エンジン詳細インタビュー)

**クラッシュ・障害回復**:

- **クラッシュ時の結果処理**: 全体をエラーとして破棄
  - ストリーミング中に50トークン生成後クラッシュしても、部分結果は返さない
  - クライアントには完全なエラーを返す（部分結果の信頼性がないため）
- **タイムアウト後の処理**: 即座にプラグインを終了（即死）
  - 30秒ウォッチドッグでタイムアウト検出後、猶予なく強制終了
  - VRAM解放のため即時終了が必要

**KVキャッシュ管理**:

- **同時リクエスト時**: リクエスト独立（isolation優先）
  - 各リクエストが独自のKVキャッシュを持つ
  - メモリ消費は大きいが、相互干渉なし
- **プレフィックス共有**: 将来検討（本仕様ではスコープ外）

**リソース監視**:

- **ウォーターマーク監視**: VRAM 90%、RAM 90% で閾値設定
  - 閾値超過時はアラート + LRUでモデルをアンロード
  - 両方同じ閾値（統一的なポリシー）
- **リーク対策**: ウォーターマーク監視で検出し、必要に応じてプラグイン再起動

**ホットリロード**:

- **要件**: 必須（ゼロダウンタイム）
  - 新プラグインをロード → 移行 → 旧プラグインをアンロード
  - サービス継続必須
- **VRAM移行戦略**: シャドウロード
  - 新旧プラグインが別モデルのようにVRAMを分け合う
  - LRUで旧が追い出される
  - 一時的に両方がメモリに存在する期間あり

**VRAM管理**:

- **パーティショニング責任**: ホスト側が統制
  - EngineHostが各プラグインにVRAM割当を指示
  - プラグインは割当て内で動作
- **VRAM不足時**: ロード拒否（エラー）
  - 必要VRAMが確保できない場合は即座にエラー
  - CPUオフロードはサポートしない
- **部分ロード**: サポートしない（GPUのみ）
  - GPUに全部載らないモデルは実行不可
  - シンプルさ優先、大モデルは使えない

**ABI互換性**:

- **ABI破壊時**: 強制マイグレーション
  - 新ABIのプラグインが入ったら、全モデルを新ABIに移行
  - 旧ABI/新ABIの共存はしない

**ストリーミング**:

- **バッファリング戦略**: トークン単位で即時送信
  - 1トークン生成ごとに即座にクライアントへ送信
  - 最小のレイテンシー

**プラグイン開発**:

- **ログ出力**: stderrをそのまま出力
  - プラグインのprintf/stderrはそのままノードのstderrに流れる
  - シンプル、特別なAPI不要
- **エラー粒度**: エラーコード＋メッセージ文字列
  - 粗粒度コード（約10種類）+ 詳細なメッセージ文字列
  - プラグインが詳細を説明
- **依存ライブラリ**: 全てバンドル（自己完結）
  - cuDNN, MKL, GGML等は全てプラグインにバンドル
  - ファイルサイズは大きいが確実に動作
- **SDK**: C ABIヘッダーのみ
  - 最小限のヘッダーファイルのみ提供
  - テストハーネスやデバッグツールは提供しない

### Session 2025-12-31 (追加インタビュー Part 2)

**マルチGPU・負荷分散**:

- **GPU選択戦略**: 自動負荷分散
  - 複数GPUがある場合、リクエストごとに空きGPUを動的に選択
  - 固定割当やユーザー指定ではなく、ホストが自動判断

**コールドスタート・ロード戦略**:

- **ロード戦略**: 遅延ロードのみ
  - 初回リクエスト時にモデルをロード
  - プリウォームやバックグラウンドプリロードは行わない
  - コールドスタートは許容

**継続バッチング詳細**:

- **バッチ戦略**: 継続バッチング（vLLM方式）
  - 生成中に新リクエストを動的に追加
- **プロンプト長混合**: プリフィル分離
  - prefillフェーズとdecodeフェーズを分離してバッチ化
  - プロンプト長が大きく異なるリクエストのパディングオーバーヘッドを回避

**メトリクス収集**:

- **収集方式**: コールバックで押し出し
  - プラグインがトークン生成ごとにホストへ通知
  - ホスト側でタイミング計測・集計

**プラグイン管理**:

- **ロード優先度**: 優先度なし（発見順）
  - ディレクトリスキャン順にプラグインをロード
  - 組み込み/サードパーティの区別なし
- **QoS**: なし（FIFO）
  - 全リクエストを到着順に処理
  - APIキーやモデルによる優先度差別化なし

**障害対策**:

- **プラグインクラッシュ時**: 自動再起動（即座）
  - クラッシュ検出後、即座に同じプラグインを再ロード
  - バックオフなし
- **メモリリーク対策**: 定期的なプラグイン再起動
  - N時間またはNリクエストごとに強制再起動
  - ウォーターマーク監視と併用

**モデル切り替え**:

- **切替レイテンシ**: 制限なし
  - モデルサイズに依存、数分でも許容
  - エラーにはしない

**アーキテクチャ宣言**:

- **サポートアーキテクチャ**: manifest.jsonに列挙
  - `architectures: ["llama", "mistral", "gemma"]` のように明示
  - ワイルドカードや動的検証は使用しない

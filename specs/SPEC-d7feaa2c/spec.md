# SPEC-d7feaa2c: Nodeエンジンローダー抽象化とNemotron直接ロード

## 背景 / 問題
Nodeは現在 llama.cpp (GGUF) を前提にしており、モデル形式の多様化（safetensors等）に対応できない。
Nemotron 3 Nano 30B A3B(BF16) はGGUF変換が失敗するため、safetensorsを直接実行できる新エンジンが必要。
ただし、Nemotron向けの新エンジン（推論エンジン）の仕様と実装は後回しにし、後で決める（TBD）。

## 目的
本仕様は統合仕様 `SPEC-3fc2c1e4`（実行エンジン）の**詳細仕様**として扱う。

- Node側でエンジンローダーを抽象化し、複数エンジンを共存可能にする
- 内蔵エンジンは **プラグイン形式（動的ロード）** とし、後から同一ABIで追加可能にする
- エンジン選択は「登録時に選択したアーティファクト（safetensors/GGUF）」と
  Hugging Face の `config.json` 等のモデル由来メタデータを正として判定する

## ゴール
- Nodeにエンジン抽象化レイヤーが導入され、llama.cpp 等の複数エンジンを選択できる
- エンジンは **動的プラグイン** として追加・更新できる
- `metadata.json` のような llm-router 独自メタデータファイルに依存せず、エンジン選択が実装される
- **対応OS/GPUの前提が明確**（macOS: Metal、Windows: DirectML、Linux: 非対応/CUDAは実験）

## アーキテクチャ（Nodeエンジン層）

### 目的
- **モデル形式や実行方式が増えても**、Node側のモデル検出・ロード・推論の責務が破綻しない構造にする。
- **“正（登録で確定したアーティファクト）” と “実行（Engine）”** を分離し、拡張を容易にする。

### コンポーネント分解（概念）

```
Router（登録・配布）                Node（取得・検証・実行）
───────────────────               ─────────────────────────
register(format=...)  ───────▶    ModelStorage
  ├─ 形式確定                           ├─ ローカル配置検出
  ├─ 必須ファイル検証                    ├─ HF由来メタデータ検証
  └─ マニフェスト確定                    └─ ModelDescriptor生成
                                           ├─ 共有パス or 外部ソース/プロキシ取得
                                           └─ GPUバックエンドに応じた取得選択
                                           │
                                           ▼
                                    EngineRegistry
                                      └─ runtime → Engine を解決
                                           │
                                           ▼
                                   EngineHost (Plugin Loader)
                                     └─ Pluginへ推論を委譲
                                           │
                                           ▼
                                       Engine Plugins（複数）
                                     ├─ llama.cpp（GGUF）
                                     └─ safetensors系（将来拡張）
```

### Key concepts
- **ModelDescriptor**: Nodeが推論を開始するために必要な最小情報（例: `format`, `runtime`, `model_dir`, `primary` など）。
- **runtime**: “どのEngineで実行すべきか” を表す識別子（例: `llama_cpp` など）。
- **Engine**: 推論の実体（GPU実行・サンプリング・ストリーミングなど）を担う差し替え可能ユニット。
- **Engine Plugin**: 共有ライブラリ + manifest.json で提供される実行ユニット。
- **Engine Host**: プラグインの発見・ロード・ライフサイクル管理のみ担当するホスト層。

### エンジン選択の入力（Single source of truth）
- **登録時に確定したアーティファクト（format/必要ファイル）**
- **Hugging Face 由来のメタデータ（`config.json` 等）**

※ Nodeローカルに複数形式が同居していることを理由に自動フォールバックは行わない（登録で確定させる）。

### OS/GPU前提（本仕様の前提）
- macOS: Apple Silicon + Metal
- Windows: DirectML（D3D12）
- Linux: 当面は非対応（CUDAは実験扱い）

## 非ゴール
- Nemotron向けの新エンジン（推論エンジン）の仕様策定・実装（後回し / TBDとして別途扱う）
- Nemotron 推論の高速化・最適化
- Python依存の導入
- GGUF変換の改善
- プラグインサンドボックス/権限分離

## ユーザーストーリー
- 開発者として、モデル形式が増えてもNodeのエンジン選択が壊れない構造にしたい

## 保留事項（TBD）
- Nemotron向けの新エンジン（推論エンジン）は別SPECとして後日仕様化する（後回し / TBD。Metal/DirectML対応方針、CUDA実験扱い、chat_template互換、dtype戦略など）。

## 受け入れ条件
- Nodeが登録時の選択（safetensors/GGUF）と `config.json` に従ってエンジンを選択する
- `metadata.json` に依存しない
- エンジン判定結果は /v1/models の応答に影響し、未対応モデルは登録対象から除外できる
- Python依存は導入しない
- Engine は動的プラグインであり、ABIバージョン一致のもののみロードされる

---

## Clarifications

### Session 2025-12-30

インタビューにより以下が確定（詳細は統合仕様 `SPEC-3fc2c1e4` を参照）:

**プラグイン実装詳細**:

- **サードパーティプラグイン**: サポートする（制限なし）
  - ディレクトリ配置のみでインストール可能
- **ABI不一致時**: ロード拒否 + アラート（通知のみ、互換レイヤーなし）
- **ID競合時**: ロードエラー（2つ目以降は拒否）
- **ネットワークアクセス**: 禁止（エンジンからの外部通信は不可）

**モダリティ対応**:

- 全モダリティ（テキスト/画像/音声）で同一Engineインターフェースを統一
- 入出力はモダリティ別メソッド（generate_text(), generate_image()等）で分離

**エンジン選択とベンチマーク**:

- 同一モデルに複数エンジンが対応可能な場合、モデル登録時のベンチマーク結果で決定
- 複合スコア（スループット + TTFT + VRAM使用率）で評価
- ベンチマーク結果はモデルメタデータに埋め込み保存

### Session 2025-12-30 (詳細インタビュー)

エンジンプラグインの詳細仕様については、統合仕様 `SPEC-3fc2c1e4` の
「Session 2025-12-30 (詳細インタビュー)」セクションを参照。

本SPECに関連する主要決定事項:

**アーキテクチャ**:

- モノリシック構成（同一プロセス内でEngine Hostとプラグインが動作）
- ハング時はノード全体を再起動（プロセス分離は将来検討）
- ウォッチドッグスレッドで30秒タイムアウト監視

**エンジン選択とロード**:

- RuntimeType競合時はベンチマーク結果で自動選択
- 遅延ロード（初回リクエスト時にロード）
- LRU順でアンロード（VRAM 90%閾値）

**GPU対応**:

- GPU別に別バイナリ（engines/{engine_id}/{gpu_backend}/{binary}）
- manifest.jsonでGPUバックエンド指定

**エラー処理**:

- GPU競合時は即座に429エラー
- VRAM OOM時はエラー返却後、次リクエストは処理継続
- クライアント切断時は即座に処理中断

### Session 2025-12-31 (内蔵エンジン詳細インタビュー)

**クラッシュ・障害回復**:

- **クラッシュ時の結果処理**: 全体をエラーとして破棄
  - ストリーミング中に50トークン生成後クラッシュしても、部分結果は返さない
  - クライアントには完全なエラーを返す（部分結果の信頼性がないため）
- **タイムアウト後の処理**: 即座にプラグインを終了（即死）
  - 30秒ウォッチドッグでタイムアウト検出後、猶予なく強制終了
  - VRAM解放のため即時終了が必要

**KVキャッシュ管理**:

- **同時リクエスト時**: リクエスト独立（isolation優先）
  - 各リクエストが独自のKVキャッシュを持つ
  - メモリ消費は大きいが、相互干渉なし
- **プレフィックス共有**: 将来検討（本仕様ではスコープ外）

**リソース監視**:

- **ウォーターマーク監視**: VRAM 90%、RAM 90% で閾値設定
  - 閾値超過時はアラート + LRUでモデルをアンロード
  - 両方同じ閾値（統一的なポリシー）
- **リーク対策**: ウォーターマーク監視で検出し、必要に応じてプラグイン再起動

**ホットリロード**:

- **要件**: 必須（ゼロダウンタイム）
  - 新プラグインをロード → 移行 → 旧プラグインをアンロード
  - サービス継続必須
- **VRAM移行戦略**: シャドウロード
  - 新旧プラグインが別モデルのようにVRAMを分け合う
  - LRUで旧が追い出される
  - 一時的に両方がメモリに存在する期間あり

**VRAM管理**:

- **パーティショニング責任**: ホスト側が統制
  - EngineHostが各プラグインにVRAM割当を指示
  - プラグインは割当て内で動作
- **VRAM不足時**: ロード拒否（エラー）
  - 必要VRAMが確保できない場合は即座にエラー
  - CPUオフロードはサポートしない
- **部分ロード**: サポートしない（GPUのみ）
  - GPUに全部載らないモデルは実行不可
  - シンプルさ優先、大モデルは使えない

**ABI互換性**:

- **ABI破壊時**: 強制マイグレーション
  - 新ABIのプラグインが入ったら、全モデルを新ABIに移行
  - 旧ABI/新ABIの共存はしない

**ストリーミング**:

- **バッファリング戦略**: トークン単位で即時送信
  - 1トークン生成ごとに即座にクライアントへ送信
  - 最小のレイテンシー

**プラグイン開発**:

- **ログ出力**: stderrをそのまま出力
  - プラグインのprintf/stderrはそのままノードのstderrに流れる
  - シンプル、特別なAPI不要
- **エラー粒度**: エラーコード＋メッセージ文字列
  - 粗粒度コード（約10種類）+ 詳細なメッセージ文字列
  - プラグインが詳細を説明
- **依存ライブラリ**: 全てバンドル（自己完結）
  - cuDNN, MKL, GGML等は全てプラグインにバンドル
  - ファイルサイズは大きいが確実に動作
- **SDK**: C ABIヘッダーのみ
  - 最小限のヘッダーファイルのみ提供
  - テストハーネスやデバッグツールは提供しない

### Session 2025-12-31 (追加インタビュー Part 2)

**マルチGPU・負荷分散**:

- **GPU選択戦略**: 自動負荷分散
  - 複数GPUがある場合、リクエストごとに空きGPUを動的に選択
  - 固定割当やユーザー指定ではなく、ホストが自動判断

**コールドスタート・ロード戦略**:

- **ロード戦略**: 遅延ロードのみ
  - 初回リクエスト時にモデルをロード
  - プリウォームやバックグラウンドプリロードは行わない
  - コールドスタートは許容

**継続バッチング詳細**:

- **バッチ戦略**: 継続バッチング（vLLM方式）
  - 生成中に新リクエストを動的に追加
- **プロンプト長混合**: プリフィル分離
  - prefillフェーズとdecodeフェーズを分離してバッチ化
  - プロンプト長が大きく異なるリクエストのパディングオーバーヘッドを回避

**メトリクス収集**:

- **収集方式**: コールバックで押し出し
  - プラグインがトークン生成ごとにホストへ通知
  - ホスト側でタイミング計測・集計

**プラグイン管理**:

- **ロード優先度**: 優先度なし（発見順）
  - ディレクトリスキャン順にプラグインをロード
  - 組み込み/サードパーティの区別なし
- **QoS**: なし（FIFO）
  - 全リクエストを到着順に処理
  - APIキーやモデルによる優先度差別化なし

**障害対策**:

- **プラグインクラッシュ時**: 自動再起動（即座）
  - クラッシュ検出後、即座に同じプラグインを再ロード
  - バックオフなし
- **メモリリーク対策**: 定期的なプラグイン再起動
  - N時間またはNリクエストごとに強制再起動
  - ウォーターマーク監視と併用

**モデル切り替え**:

- **切替レイテンシ**: 制限なし
  - モデルサイズに依存、数分でも許容
  - エラーにはしない

**アーキテクチャ宣言**:

- **サポートアーキテクチャ**: manifest.jsonに列挙
  - `architectures: ["llama", "mistral", "gemma"]` のように明示
  - ワイルドカードや動的検証は使用しない

### Session 2025-12-31 (追加インタビュー Part 3)

**セキュリティ**:

- **プラグイン信頼モデル**: 信頼前提（何もしない）
  - プラグインは信頼できる前提でロード
  - 署名検証・サンドボックス・リソース制限なし
  - ユーザー責任で管理

**推論キャッシュ**:

- **キャッシュ戦略**: 同一プロンプトをキャッシュ
  - 温度0の場合のみ、同一プロンプトの結果をキャッシュ
  - キャッシュヒット時は推論をスキップ
- **ストレージ**: インメモリLRU
  - ノードプロセス内のメモリに保持
  - 再起動で消失（永続化なし）
- **サイズ上限**: RAM割合で制限
  - 利用可能RAMの一定割合（例: 5%）をキャッシュに割当

**リクエスト復旧**:

- **クラッシュ後の処理**: 自動リトライ
  - クライアントに透過的にリトライ実行
  - 重複処理リスクはあるが、ユーザー体験を優先
- **リトライ戦略**: 指数バックオフ
  - 失敗ごとに待機時間を倍増
  - 固定パラメータ（初期値100ms、最大4回、上限30秒程度）

**並行処理**:

- **モデル並行ロード**: 許可
  - 複数GPUがある場合、複数モデルを同時にVRAMにロード可能
  - VRAM空き容量の範囲内で並行実行
- **プラグインインスタンス**: 単一インスタンス
  - 1つのプラグインは1つのみロード
  - 内部で複数モデル・アーキテクチャを管理

**キャンセル処理**:

- **応答速度**: 即座に中断
  - 次のトークン生成前に必ず中断
  - キャンセル処理は最優先
- **バッチ影響**: 他リクエストへの影響なし
  - キャンセルされたリクエストのみ停止
  - 同一バッチ内の他リクエストは継続

**メモリ管理**:

- **フラグメンテーション対策**: 定期再起動で対処
  - 既存の定期再起動ポリシーで十分
  - 追加のメモリプール機構は不要

### Session 2025-12-31 (追加インタビュー Part 4)

**モデルライフサイクル**:

- **ウォームアップ**: 初回リクエスト時に許容
  - 起動時の自動ウォームアップは行わない
  - 初回リクエストの遅延は許容する
- **並行リクエスト上限**: 上限なし（キュー制御のみ）
  - モデルごとの同時リクエスト数制限は設けない
  - リクエストはキューで待機
- **ロールバック**: 不要（リクエスト単位で完結）
  - 生成途中のトークン列を部分破棄する機能は提供しない
  - KVキャッシュの巻き戻しは不要
- **コンテキスト長超過**: エラーで拒否
  - 上限を超えた場合は即座にエラー
  - sliding windowやサマリー圧縮は行わない

**パラメータ処理**:

- **サンプリングパラメータ検証**: Nodeで検証
  - temperature/top_p/top_k等の範囲チェックはNodeが実施
  - 不正値は早期にエラー
- **stop sequences**: Node/プラグイン内で検出
  - 生成ループ内でトークン列をマッチング
  - 一致時に生成を終了
- **logprobs**: サポートする
  - トークン確率の返却をサポート
  - top-N確率を返却可能
- **空プロンプト**: エラーで拒否
  - 空または空白のみのプロンプトは400 Bad Request

**システム運用**:

- **VRAMリーク対策**: 定期再起動で対応
  - 異常終了時のVRAMリークは定期再起動ポリシーでクリア
  - 専用のクリーンアップAPIは不要
- **ヘルスチェック**: Node単位
  - liveness/readinessはNode全体で判定
  - モデル個別のヘルスエンドポイントは提供しない
- **ログ統合**: stderrをホストがキャプチャ
  - プラグインのstderrをNode側でキャプチャ
  - 統合ログに出力
- **アンロード判断**: LRU（最後に使用された順）
  - 最も古いアクセスのモデルから優先的にアンロード

**互換性・検証**:

- **未対応パラメータ**: エラーで拒否
  - モデルが対応していない機能（vision用画像等）はエラー
  - 黙殺せず明示的に拒否
- **max_tokensデフォルト**: max_position_embeddings - プロンプト長
  - 未指定時はモデルの最大コンテキスト長からプロンプト長を差し引いた残りを使用
- **アーキテクチャ不一致**: ロード前にチェックし拒否
  - manifestのarchitecturesとconfig.jsonを照合
  - 不一致時はロード開始前にエラー
- **フォーマット対応**: 統合（単一プラグインが両対応）
  - 1つのプラグインがGGUF/safetensors両方をサポート可能
  - フォーマットごとに別プラグインを用意する必要なし

### Session 2025-12-31 (追加インタビュー Part 5)

**量子化・モデル指定**:

- **量子化選択**: 全量子化対応、`modelname:量子化` フォーマットで指定
  - 例: `llama-7b:Q4_K_M`
  - 同一モデルで複数量子化ファイルがある場合、指定必須
- **量子化指定形式**: 完全一致のみ
  - 大文字/小文字、ハイフン/アンダースコアの揺れは許容しない
  - `Q4_K_M` と `q4-k-m` は別として扱う
- **VRAM不足時（大規模モデル）**: ロード拒否
  - CPU/GPUスプリットやテンソル並列は行わない
  - GPUに収まらないモデルはエラー
- **モデルバージョン管理**: サポートしない
  - 同一モデル名で複数バージョンの共存は不可
  - 更新時は上書き

**高度な推論機能**:

- **Speculative Decoding**: サポートしない
  - ドラフト/ターゲットモデル協調は複雑すぎるためスコープ外
- **Prefix Caching**: サポートする
  - 同一システムプロンプトのKVキャッシュをリクエスト間で共有
  - TTFT削減に有効
- **Prefix Cache上限**: VRAM割合で設定
  - モデルロード後の空きVRAMのN%をキャッシュに割当

**Vision/マルチモーダル**:

- **mmproj管理**: 自動検出
  - 同一ディレクトリ内のmmproj（画像エンコーダ）ファイルを自動検出・ロード
- **Embedding対応**: 同一インターフェース
  - Completion/Embeddingモデルで同じプラグインAPIを使用
  - manifestでモード（completion/embedding）を宣言

**スケーラビリティ**:

- **レプリカ配置**: サポートする
  - 同一モデルを複数GPUにロードしてスループット向上
  - ホストがGPU間で負荷分散
- **負荷分散方式**: ラウンドロビン
  - 複数レプリカ間で順番にリクエストを振り分け

**トークン処理**:

- **トークンカウント**: Node側で実施
  - 推論前のトークン数予測はNodeが担当
  - モデルのtokenizerを使用
- **Function Calling**: Node/プラグイン側で実装
  - ツール呼び出しの検出・JSON生成はNode/プラグイン内
- **chat_template**: Node側でレンダリング
  - Jinjaテンプレートの適用はNodeが担当
  - Jinjaライブラリ: inja（C++ライブラリ）

**エラー処理・その他**:

- **タイムアウト時出力**: ストリーム時のみ部分出力
  - ストリームモード: タイムアウトまでに生成されたトークンを返却
  - 非ストリームモード: エラーのみ返却
- **ダウンロード中リクエスト**: 即座にエラー
  - モデルダウンロード中は503 Service Unavailable
  - キュー待機せず即座に拒否
- **モデル登録**: 手動登録のみ
  - ファイルシステム直接追加では登録されない
  - CLI等で明示的な登録コマンドが必要
- **ライセンス情報**: manifest.jsonに記載
  - 商用利用可否等のライセンス情報をプラグインマニフェストに含める
- **推論ログ保存**: 保存しない
  - プロンプト・応答・トークン数のログは保存しない
  - メトリクス収集のみ

### Session 2025-12-31 (追加インタビュー Part 6)

**VRAM管理（詳細）**:

- **部分ロード中のVRAM不足**: 即座に全解放してエラー
  - ロード途中でVRAMが不足した場合、ロード済み部分を即座に解放
  - 部分状態を保持せず、クリーンな状態に戻す
  - エラーを返却してリクエストを終了

**量子化選択（詳細）**:

- **自動アップグレード**: 行わない（常に指定通り）
  - VRAMに余裕があっても高精度版への自動アップグレードは行わない
  - ユーザーが指定した量子化を厳密に使用
  - 予測可能性と一貫性を優先

**障害回復（詳細）**:

- **クラッシュ後のリクエスト処理**: 即座に503返却
  - プラグインクラッシュ後、再起動完了を待たずに即座にエラー
  - クライアントはリトライで対応
  - 再起動中のキュー待機は行わない

**ストリーミング（詳細）**:

- **ハング検出**: 最終トークンから5秒後に強制終了
  - トークン生成が5秒間停止した場合をハングと判定
  - 30秒のリクエスト全体タイムアウトとは別に監視
  - ストリーミング中の応答停滞を早期検出

**ログ管理（詳細）**:

- **プラグインログ統合**: ホストログにプラグインIDを付与して統合
  - プラグインのstdoutもstderrもホストがキャプチャ
  - 各ログ行にプラグインIDのプレフィックスを付与
  - 複数プラグイン稼働時も識別可能

**API（詳細）**:

- **ロード進捗API**: 公開しない
  - モデルロード中の進捗率は外部に公開しない
  - ロード完了/失敗のみを通知
  - 実装シンプル化を優先

**モダリティ処理（詳細）**:

- **モダリティ優先度**: リクエスト順（到着順）
  - Completion/Embeddingで優先度の差なし
  - 全リクエストをFIFOで処理
  - モダリティによる優先度差別化なし

# 機能仕様書: ノードベースモデル管理とモデル対応ルーティング


## 用語

- 本仕様の「ノード」は、xLLMランタイム（推論サーバー）を指す。

**機能ID**: `SPEC-93536000`
**作成日**: 2026-01-03
**更新日**: 2026-01-25
**ステータス**: 改定中（自動認識への移行）

## ユーザーシナリオ&テスト

### ユーザーストーリー1 - モデル対応ノードへの正確なルーティング (優先度: P1)

ユーザーがOpenAI互換APIでチャットリクエストを送信すると、システムは指定されたモデルを
実行可能なノードにのみリクエストをルーティングする。これにより、Metal専用モデルが
CUDAノードに送られたり、CUDA専用モデルがmacOSノードに送られるといった問題を防ぐ。

**この優先度の理由**: ルーティングの正確性はシステムの根幹機能であり、誤ったルーティングは
推論失敗を引き起こす致命的な問題となる。

**独立テスト**: Metal専用モデルへのリクエストがmacOSノードにのみルーティングされ、
CUDAノードには決して送られないことをテストできる。

**受け入れシナリオ**:

1. **前提** macOSノード（Metal）とLinuxノード（CUDA）が登録されている、
   **実行** Metal専用モデルへのリクエストを送信、
   **結果** リクエストはmacOSノードにのみルーティングされる

2. **前提** CUDAノードのみがオンラインでMetal専用モデルをリクエスト、
   **実行** チャットリクエストを送信、
   **結果** 503 Service Unavailableエラーが返される（対応ノードなし）

3. **前提** 複数のmacOSノードがオンライン、
   **実行** Metal専用モデルへのリクエストを送信、
   **結果** 負荷分散ロジックに従い、対応ノードの中から1つが選択される

---

### ユーザーストーリー2 - 利用可能モデル一覧の動的取得 (優先度: P1)

ユーザーが`/v1/models`エンドポイントを呼び出すと、現在オンラインのノードが
実行可能なモデルのみが返される。オフラインのノードが持つモデルは含まれない。

**この優先度の理由**: ユーザーは利用可能なモデルを事前に確認し、存在しないモデルへの
リクエストを避ける必要がある。

**独立テスト**: ノードのオンライン/オフライン状態を変更し、`/v1/models`の
レスポンスが動的に変化することをテストできる。

**受け入れシナリオ**:

1. **前提** ノードAとノードBがオンラインで異なるモデルセットを持つ、
   **実行** `/v1/models`を呼び出す、
   **結果** 両ノードの実行可能モデルが統合されて返される

2. **前提** ノードAがオフラインになった、
   **実行** `/v1/models`を呼び出す、
   **結果** ノードAのモデルはリストに含まれない

3. **前提** すべてのノードがオフライン、
   **実行** `/v1/models`を呼び出す、
   **結果** 空のモデルリストが返される

---

### ユーザーストーリー3 - 存在しないモデルへの明確なエラー (優先度: P2)

ユーザーが`/v1/models`に含まれていないモデルを指定してリクエストを送信すると、
明確な404 Model Not Foundエラーが返される。

**この優先度の理由**: ユーザーが誤ったモデル名を指定した場合、即座にフィードバックを
得られることで、デバッグ時間を短縮できる。

**独立テスト**: 存在しないモデル名でリクエストを送信し、404エラーが返ることをテストできる。

**受け入れシナリオ**:

1. **前提** オンラインノードが存在する、
   **実行** 存在しないモデル名でリクエストを送信、
   **結果** 404 Model Not Foundエラーが返される

2. **前提** モデル"llama2-7b"がオンラインノードで利用可能、
   **実行** "llama2-7b"でリクエストを送信、
   **結果** 正常にルーティングされる

---

### ユーザーストーリー4 - ノード側でのGPU互換モデル報告 (優先度: P1)

ノードは起動時に自身のGPUバックエンド（Metal/CUDA/DirectML/ROCm）を検出し、
そのGPUで実行可能なモデル一覧をロードバランサーに報告する。

**この優先度の理由**: これがシステム全体の基盤となる。ノードが正確にモデル対応を
報告しなければ、ルーティングの正確性は保証できない。

**独立テスト**: macOSノードを起動し、Metal対応モデルのみがロードバランサーに報告されることを
テストできる。

**受け入れシナリオ**:

1. **前提** macOSノードが起動、
   **実行** ロードバランサーに登録、
   **結果** ノードはMetal対応モデルのみをexecutable_modelsとして報告

2. **前提** LinuxノードがNVIDIA GPUを検出、
   **実行** ロードバランサーに登録、
   **結果** ノードはCUDA対応モデルをexecutable_modelsとして報告

3. **前提** GPU非搭載ノード、
   **実行** ロードバランサーに登録を試みる、
   **結果** 登録が拒否される（既存の仕様通り）

---

### エッジケース

- モデルが複数のGPUバックエンドに対応している場合、すべての対応ノードが候補となる
- ノードのexecutable_modelsは起動時に固定され、ランタイムでは変化しない
- ノードが応答しなくなった場合、そのノードのモデルは`/v1/models`から除外される
- 推論失敗時は該当モデルのみを除外（ノード全体ではない）、復帰にはノード再起動が必要
- ロードバランサー起動直後でノードが未登録の場合、503を即座に返す（キューイングしない）
- 複数GPUを持つノードはプライマリGPUのみを使用する
- GPU優先度による選択は行わない（負荷分散ロジックのみで選択）
- ノードの全モデルが除外された場合、ルーティング対象外となるがステータスはオンラインのまま
- ノード登録後にモデルファイルが削除された場合、推論失敗で通常の除外フローが発動
- 不正なJSONレスポンスの場合は登録を拒否、有効なJSONで一部エントリが不正な場合は有効分のみで登録
- ノード再登録（同一ノードID）時はexcluded_modelsをクリアし、新しいモデル一覧で更新

## 要件

### 機能要件

- **FR-001**: ノードは起動時にGPUバックエンド（Metal/CUDA/DirectML/ROCm/CPU）を自動検出する
- **FR-002**: ノードはGPU互換性に基づいて実行可能なモデル一覧を決定する（起動時に固定）
- **FR-003**: ノードは`/v1/models`エンドポイントで実行可能モデルIDのみを返す（GPU情報は不要）
- **FR-004**: ロードバランサーはノード登録時に`/v1/models`を呼び出してモデル一覧を取得する（プル型）
- **FR-005**: ロードバランサーはオンラインノードの実行可能モデルを集約して`/v1/models`で返す
- **FR-006**: ロードバランサーはリクエスト受信時、指定モデルを実行可能なノードにのみルーティングする
- **FR-007**: 対応ノードがない場合、503 Service Unavailableを返す
- **FR-008**: `/v1/models`にないモデル指定時は404 Model Not Foundを返す
- **FR-009**: ~~ノードはsupported_models.jsonをビルド時に埋め込み、GPU互換性判定に使用する~~ **廃止**: ノードはHuggingFace config.jsonからアーキテクチャを自動検出し、GPU互換性を判定する
- **FR-010**: ロードバランサー側のsupported_models.json（REGISTERED_MODELS）は完全に削除する
- **FR-011**: モデルIDは完全一致（大文字小文字区別）でマッチングする
- **FR-012**: 推論失敗時、当該ノードから該当モデルを一時的に除外する（1回で即除外）
- **FR-013**: モデル除外からの復帰はノード再起動のみで行う
- **FR-014**: /v1/models取得失敗または空リストの場合、ノード登録を拒否する
- **FR-015**: ノードがオフライン復帰時、/v1/modelsを再取得してモデル一覧を更新する
- **FR-016**: supported_models.jsonでplatformsが未定義のモデルは除外する
- **FR-017**: 推論エラーの種類に関係なく、全エラーでモデル除外をトリガーする
- **FR-018**: ノードの/v1/modelsにはモデルファイルが存在するモデルのみを含める
- **FR-019**: ノードの/v1/modelsレスポンスの各モデルエントリには「id」フィールドのみ必須
- **FR-020**: 不正なモデルエントリ（ID空など）はスキップし、有効なモデルのみで登録を継続する
- **FR-021**: ノード内で同じモデルIDが重複して報告された場合は重複排除する
- **FR-022**: ノード再登録時にexcluded_modelsをクリアする（除外状態のリセット）
- **FR-023**: モデル除外は新規リクエストのみに影響し、進行中リクエストは継続処理する
- **FR-024**: ~~supported_models.jsonに存在しないモデルファイルは/v1/modelsに含めない~~ **廃止**: モデルファイルが存在し、config.jsonからアーキテクチャを認識できれば/v1/modelsに含める

### 自動認識要件（新規追加 2026-01-25）

- **FR-025**: ノードはモデルディレクトリ内の`config.json`を読み込み、`architectures`フィールドからアーキテクチャを検出する
- **FR-026**: 検出したアーキテクチャは`normalize_architecture()`で正規化し、対応エンジンを解決する
- **FR-027**: 対応エンジンが存在しないアーキテクチャのモデルは/v1/modelsに含めない（未対応モデルの自動除外）
- **FR-028**: `config.json`が存在しない、または`architectures`フィールドがないモデルはWARNログを出力し、/v1/modelsに含めない
- **FR-029**: GGUFファイルの場合、ファイル内のメタデータからアーキテクチャを検出する（llama.cppの既存機能を活用）
- **FR-030**: safetensorsファイルの場合、同一ディレクトリの`config.json`からアーキテクチャを検出する

### ログ出力要件

- **LOG-001**: モデル除外時はWARNレベルで「ノードXのモデルYを除外」を出力する
- **LOG-002**: ノード登録拒否時はERRORレベルで理由を出力する
- **LOG-003**: モデル一覧取得成功時はDEBUGレベルで一覧を出力する
- **LOG-004**: platforms未定義のモデルはWARNレベルで警告を出力する

### 廃止要件

- **DEPRECATED-001**: SPEC-dcaeaec4のFR-9「全ノード全モデル対応の原則」を完全廃止する
- **DEPRECATED-002**: ロードバランサーの中央集権的なモデル管理（supported_models.json）を廃止する

### 主要エンティティ

- **GpuBackend**: ノードのGPU種別（Metal, CUDA, DirectML, ROCm, CPU）※ノード側のみ
- **Node.executable_models**: ノードが実行可能なモデルID一覧（ロードバランサー側で管理）
- **Node.excluded_models**: 推論失敗により一時除外中のモデルID一覧（ロードバランサー側メモリのみ）

## スコープ外

以下の機能は本仕様のスコープ外とし、将来のバージョンで対応予定:

- VRAMサイズに基づく動的モデルフィルタリング
- モデルの動的ダウンロード・アンロード連携
- 複数GPUを持つノードの個別GPU指定
- ダッシュボードUIでのexecutable_models表示
- フィーチャーフラグによる段階的ロールアウト

## 技術制約

- ノードの/v1/modelsが空または取得失敗の場合、登録を拒否する
- 後方互換性は不要（旧バージョンノードはサポートしない）
- executable_modelsはメモリのみで管理（DB永続化なし）
- ロードバランサーはGPUバックエンド情報を保持しない（ノード側のみ）
- /v1/models取得のタイムアウトは5秒（ヘルスチェックと同じ）

## 前提条件

この機能は以下を前提とします:

- ~~ノードはビルド時にsupported_models.jsonを組み込み済み~~ **廃止**
- ~~各モデルにはplatforms属性（対応GPUバックエンドリスト）が定義されている~~ **廃止**
- モデルディレクトリには`config.json`（HuggingFace形式）または有効なGGUFファイルが存在する
- `config.json`には`architectures`フィールドが含まれる（HuggingFace標準形式）
- ノードの`engine_registry`には対応アーキテクチャとエンジンのマッピングが登録されている

## 依存関係

この機能は以下に依存します:

- SPEC-dcaeaec4: LLM-Load Balancer独自モデルストレージ（FR-9を廃止）
- SPEC-05098000: リクエストキューイングシステム（ルーティングロジック拡張）

## 成功基準

以下の成功基準を満たす必要があります:

1. Metal専用モデルへのリクエストがCUDAノードにルーティングされることは決してない
2. CUDA専用モデルへのリクエストがmacOSノードにルーティングされることは決してない
3. `/v1/models`はオンラインノードの実行可能モデルのみを返す
4. 対応ノードがないモデルへのリクエストは503エラーで即座に拒否される
5. 存在しないモデルへのリクエストは404エラーで即座に拒否される
6. ノードのオンライン/オフライン変更は10秒以内に`/v1/models`に反映される

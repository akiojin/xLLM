# 機能仕様書: safetensors.cpp

**機能ID**: `SPEC-69549000`
**作成日**: 2026-01-06
**ステータス**: 実装完了
**入力**: ユーザー説明: "safetensors.cpp: llama.cpp/stable-diffusion.cpp/whisper.cppと同様の独立C++プロジェクト。ggmlバックエンドを利用し、safetensors形式のLLM（gpt-oss、Nemotron 3含む）をMetal/CUDA/ROCm/Vulkanで推論可能にする。"

## ユーザーシナリオ＆テスト

### ユーザーストーリー1 - safetensors形式のモデルで推論を実行したい (優先度: P1)

開発者として、HuggingFaceで公開されているsafetensors形式のLLMモデルを、GGUFに変換することなく直接読み込んで推論を実行したい。これにより、モデル変換の手間を省き、最新のモデルをすぐに利用できるようになる。

**この優先度の理由**: safetensors形式はHuggingFaceの標準フォーマットであり、多くの最新モデルがこの形式で公開されている。GGUF変換なしで直接利用できることが本プロジェクトの核心的な価値である。

**独立テスト**: safetensorsファイルを読み込み、テキスト生成を実行することで完全にテスト可能。モデル変換なしでの推論という価値を提供する。

**受け入れシナリオ**:

1. **前提** OpenAI公式のgpt-oss-20b safetensorsモデル（openai/gpt-oss-20b, MoE + MXFP4）、**実行** モデルをロードしてプロンプトを入力、**結果** テキストが生成される
2. **前提** NVIDIA公式のNemotron 3 safetensorsモデル（nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16）、**実行** モデルをロードしてプロンプトを入力、**結果** テキストが生成される
3. **前提** 分割されたsafetensors（model-00001-of-00005.safetensors等）、**実行** index.jsonを解析してロード、**結果** 全テンソルが正しく読み込まれる
4. **前提** 不正なsafetensorsファイル、**実行** モデルをロード、**結果** 明確なエラーメッセージで失敗する

**重要**: テストは公式が配布するsafetensors版モデルを使用すること。サードパーティのGGUF変換版でのテストは不十分。

---

### ユーザーストーリー2 - 複数のGPUプラットフォームで同一コードを実行したい (優先度: P1)

運用者として、macOS（Metal）、Windows/Linux（NVIDIA CUDA）、Linux（AMD ROCm）、クロスプラットフォーム（Vulkan）のいずれの環境でも、同じライブラリを使用してsafetensorsモデルの推論を実行したい。

**この優先度の理由**: GPUベンダーごとに異なるコードを保守する負担を避け、統一されたインターフェースで全プラットフォームに対応することが運用効率の観点から重要。

**独立テスト**: 各GPUプラットフォームで同一のモデルとプロンプトを使用して推論を実行し、結果が一致することを確認。

**受け入れシナリオ**:

1. **前提** Apple Silicon Mac環境、**実行** Metal バックエンドでモデルをロードして推論、**結果** GPUを使用してテキストが生成される
2. **前提** NVIDIA GPU搭載環境、**実行** CUDA バックエンドでモデルをロードして推論、**結果** GPUを使用してテキストが生成される
3. **前提** AMD GPU搭載Linux環境、**実行** ROCm/HIP バックエンドでモデルをロードして推論、**結果** GPUを使用してテキストが生成される
4. **前提** Vulkan対応GPU環境、**実行** Vulkan バックエンドでモデルをロードして推論、**結果** GPUを使用してテキストが生成される

---

### ユーザーストーリー3 - リアルタイムでトークン生成を受け取りたい (優先度: P1)

エンドユーザーとして、チャットUIでAIの応答をリアルタイムで確認したい。長い応答の生成を待たずに、トークン単位で徐々に表示されることで、対話体験が向上する。

**この優先度の理由**: OpenAI互換APIのstream=trueをサポートするために必須。ユーザー体験に直結する機能である。

**独立テスト**: ストリーミングモードで推論を実行し、トークンごとにコールバックが呼ばれることを確認。

**受け入れシナリオ**:

1. **前提** ストリーミングモードを有効化、**実行** テキスト生成を開始、**結果** トークンごとにコールバックが呼ばれる
2. **前提** ストリーミングモードを無効化、**実行** テキスト生成を開始、**結果** 完了時に一括で結果が返される

---

### ユーザーストーリー4 - 複数リクエストを効率的に処理したい (優先度: P1)

運用者として、複数のユーザーからの同時リクエストを効率的に処理したい。GPUリソースを最大限活用し、スループットを向上させたい。

**この優先度の理由**: 本番環境での利用において、continuous batchingによる効率的なリソース利用は必須要件。

**独立テスト**: 複数の推論リクエストを同時に発行し、バッチ処理されることを確認。

**受け入れシナリオ**:

1. **前提** 複数の推論リクエストがキューイング、**実行** バッチ処理を実行、**結果** 各リクエストに対して正しい結果が返される
2. **前提** 異なる長さのプロンプト、**実行** continuous batching、**結果** 完了したものから順次結果が返される

---

### ユーザーストーリー5 - 独立したライブラリとして利用・配布したい (優先度: P2)

プロジェクト管理者として、safetensors.cppを他のプロジェクトに依存しない独立したライブラリとして管理し、将来的に別リポジトリとして公開・配布したい。

**この優先度の理由**: llama.cpp、stable-diffusion.cpp、whisper.cppと同様に、コミュニティで広く利用される独立プロジェクトとして成長させるため。

**独立テスト**: safetensors.cpp単体でビルド・テストが完結し、外部プロジェクト（Router/xLLM）への依存がないことを確認。

**受け入れシナリオ**:

1. **前提** safetensors.cppディレクトリのみ、**実行** 単体でビルド、**結果** 外部依存なしでビルドが成功する
2. **前提** safetensors.cppディレクトリのみ、**実行** 単体でテスト実行、**結果** すべてのテストがパスする
3. **前提** 別のC++プロジェクト、**実行** safetensors.cppをサブモジュールとして追加、**結果** ライブラリとして利用できる

---

### ユーザーストーリー6 - テキスト埋め込みを生成したい (優先度: P2)

開発者として、テキストの埋め込みベクトルを生成し、検索や類似度計算に利用したい。

**この優先度の理由**: RAGやセマンティック検索などの用途でembedding APIは重要な機能。

**独立テスト**: 埋め込みモデルをロードし、テキストからベクトルを生成できることを確認。

**受け入れシナリオ**:

1. **前提** 埋め込み対応モデルがロード済み、**実行** テキストを入力、**結果** 埋め込みベクトルが返される
2. **前提** 複数のテキスト、**実行** バッチで埋め込み生成、**結果** 各テキストのベクトルが返される

---

### ユーザーストーリー7 - 構造化された出力を得たい (優先度: P2)

開発者として、AIの出力を特定のJSON形式に制約したり、ツール呼び出しの形式で取得したい。

**この優先度の理由**: OpenAI互換APIのtools/function_calling対応のため。アプリケーション統合に重要。

**独立テスト**: response_formatやtoolsを指定して推論を実行し、指定形式で出力されることを確認。

**受け入れシナリオ**:

1. **前提** response_format: json_objectを指定、**実行** 推論を実行、**結果** 有効なJSONが返される
2. **前提** toolsを定義、**実行** ツール呼び出しを促すプロンプト、**結果** tool_callsが返される

---

### ユーザーストーリー8 - 新しいモデルアーキテクチャを追加したい (優先度: P3)

開発者として、gpt-oss-20b/nemotron以外の新しいモデルアーキテクチャをプラグインとして追加できるようにしたい。

**この優先度の理由**: 将来的な拡張性を確保するため。Llama系、Mistral、Qwen等への対応を可能にする。

**独立テスト**: 新しいアーキテクチャプラグインを追加し、対応モデルで推論できることを確認。

**受け入れシナリオ**:

1. **前提** 新しいアーキテクチャプラグインを実装、**実行** プラグインを登録、**結果** 対応モデルがロード可能になる
2. **前提** 未対応アーキテクチャのモデル、**実行** ロードを試行、**結果** 未対応である旨のエラーが返される

---

### エッジケース

- safetensorsファイルが破損している場合、明確なエラーメッセージで失敗する
- モデルがVRAMに収まらない場合、VRAM不足エラーを返す（自動オフロードは行わない）
- 対応していないモデルアーキテクチャの場合、未対応である旨のエラーを返す
- GPUが検出されない場合、明確なエラーを返す（CPU推論は初期スコープ外）
- config.jsonがない場合、safetensorsからアーキテクチャを推測試行する
- 分割されたsafetensorsでindex.jsonがない場合、ファイル名パターンから推測する
- 推論中にキャンセルが要求された場合、安全に中断して部分結果を返す
- VRAM見積もりがロード前に失敗を予測した場合、明確なエラーで早期リターンする
- mmapがサポートされない環境では通常のファイル読み込みにフォールバックする

## 要件

### 機能要件

- **FR-001**: safetensors形式のモデルファイルを直接読み込み、推論を実行できる
- **FR-002**: 分割されたsafetensors（model-00001-of-00005.safetensors等）を結合ロードできる
- **FR-003**: model.safetensors.index.jsonを解析してテンソルマッピングを取得できる
- **FR-004**: Metal（Apple Silicon）でGPU推論を実行できる
- **FR-005**: CUDA（NVIDIA）でGPU推論を実行できる
- **FR-006**: ROCm/HIP（AMD）でGPU推論を実行できる
- **FR-007**: Vulkan（クロスベンダー）でGPU推論を実行できる
- **FR-008**: 初期対応アーキテクチャ（下記参照）をサポートする
- **FR-009**: アーキテクチャをプラグイン形式で拡張できる
- **FR-011**: FP16/BF16モデルをロードできる
- **FR-012**: ggml量子化形式（Q4_K_M等）をランタイムで適用できる
- **FR-013**: 事前量子化モデル（GPTQ/AWQ）を読み込める
- **FR-014**: トークン単位のストリーミング出力をサポートする
- **FR-015**: continuous batchingで複数リクエストを効率的に処理できる
- **FR-016**: embeddings APIをサポートする
- **FR-017**: tools/function_calling（構造化出力）をサポートする
- **FR-018**: llama.cpp互換のC APIを提供する
- **FR-019**: 外部プロジェクト（Router/xLLM）に依存せず、単体でビルド・テストできる
- **FR-020**: ggmlをサブモジュールとして管理し、GPUバックエンドとして利用する
- **FR-021**: HuggingFaceのtokenizer.jsonを解析してトークナイズできる
- **FR-022**: HuggingFaceのconfig.jsonからモデル設定を読み込める
- **FR-023**: config.jsonがない場合、safetensorsからアーキテクチャを推測試行する
- **FR-024**: temperature, top_p, top_k等のサンプリングパラメータをサポートする
- **FR-025**: メモリマップドI/O（mmap）でsafetensorsを高速ロードできる
- **FR-026**: 推論中のキャンセル（中断）機能をサポートする
- **FR-027**: DEBUG/INFO/WARN/ERRORの設定可能なログレベルをサポートする
- **FR-028**: モデルロード前にVRAM必要量を事前見積もりできる
- **FR-029**: KVキャッシュの量子化（INT8/FP8）をサポートする
- **FR-030**: Rope Scaling（Linear/NTK）をサポートする
- **FR-031**: Sliding Window Attentionをサポートする
- **FR-032**: Grouped Query Attention（GQA）/Multi-Query Attention（MQA）をサポートする
- **FR-033**: モデルのウォームアップ時間を計測・ログ出力する
- **FR-034**: Speculative Decoding（投機的デコーディング）をサポートする（将来対応）
- **FR-035**: 全てのAPIをスレッドセーフで設計する
- **FR-036**: 最大コンテキスト長はconfig.jsonのmax_position_embeddingsに従う
- **FR-037**: LoRA/QLoRAアダプターのロードをサポートする
- **FR-038**: LoRAアダプターのホットリロード（動的切り替え）をサポートする
- **FR-039**: tokenizer_config.jsonのchat_template（Jinja2形式）を解析・適用できる
- **FR-040**: Pipeline Parallelismによるマルチ GPU分散推論をサポートする
- **FR-041**: プロンプトキャッシュ（システムプロンプト/RAG等のKV再利用）をサポートする
- **FR-042**: モデルのプリフィル（プロンプト処理）を最適化してTTFTを短縮する
- **FR-043**: Visionモデル（LLaVA、Qwen-VL等）の画像入力をサポートする（将来対応）
- **FR-044**: gpt-ossのMoEルーティング（Top-K）と専門家FFNを実装し、gpt-ossのsafetensorsを直接推論できる
- **FR-045**: gpt-ossのMXFP4ブロック量子化（blocks + scales）をggml形式に変換し、推論に使用できる

### 対応アーキテクチャ一覧（実装反映・追記式）

この表が「現時点での対応状況」の唯一の正とする。新しいアーキテクチャを追加した場合は、
必ずここに**追記**し、READMEにも反映すること。

| アーキテクチャ | 状態 | 実装根拠 | 備考 |
|--------------|------|----------|------|
| **gpt-oss (MoE + MXFP4)** | 実装済み | `xllm/third_party/safetensors.cpp/src/ggml_model.cpp`, `xllm/third_party/safetensors.cpp/src/transformer.cpp` | `mlp.router.*` / `mlp.experts.*_(blocks\|scales\|bias)` の読み込みとMoE forwardを含む |
| **nemotron3 (Mamba-Transformer MoE)** | 実装済み（未統合） | `xllm/third_party/safetensors.cpp/src/arch/nemotron3.*`, `mamba.*`, `moe.*`, `gqa.*` | forwardパスとの統合が未実施 |

**備考**:

- `model_type`/`architectures` の検出は実装済みだが、専用の重みマッピングや forward パスがないアーキテクチャは**対応と見なさない**。
- 対応対象を増やす場合は、専用のテンソル名マッピング・forwardパス・E2E検証をセットで追加する。

### 将来候補（未対応）

以下は将来候補としての例であり、現時点の対応を示すものではない。

- llama / mistral / qwen / phi / gemma / glm など

### 主要エンティティ

- **Model**: safetensors形式のLLMモデル。アーキテクチャ情報、重みテンソル、設定を含む
- **Context**: 推論実行のためのコンテキスト。GPUバックエンド、メモリ管理、KVキャッシュを保持
- **Backend**: GPU計算バックエンド。Metal/CUDA/ROCm/Vulkanのいずれか
- **Tokenizer**: HuggingFace互換のトークナイザー。tokenizer.jsonを解析
- **Sampler**: 次トークン選択のロジック。temperature, top_p, top_k等を適用
- **Architecture**: モデルアーキテクチャの定義。プラグイン形式で拡張可能
- **Batch**: continuous batching用のリクエストバッチ

---

## スコープ外

- CPU推論（GPU必須。CPU対応は将来検討）
- GGUFフォーマットへの対応（llama.cppが担当）
- モデルの学習・ファインチューニング機能
- Webサーバー機能（xLLM側で提供）
- モデルのダウンロード・管理機能（Router側で提供）
- VRAM不足時のGPU/CPU自動分割オフロード

---

## 技術制約

- GPU搭載環境が必須（CPU推論は初期スコープ外）
- ggmlライブラリ（本家リポジトリ）のGPUバックエンド対応状況に依存
- C++17以上のコンパイラが必要
- ライセンス: MIT

---

## 前提条件

- 対象GPUに対応したドライバがインストールされている
- safetensors形式のモデルファイルが利用可能である
- モデルがVRAMに収まるサイズである

---

## 依存関係

- ggml（サブモジュールとして管理、GPUバックエンド提供、本家リポジトリに追従）
- SPEC-d7feaa2c（エンジンローダー）- Runtime統合時にプラグインアーキテクチャに準拠
- SPEC-3fc2c1e4（実行エンジン統合仕様）- Runtime統合時に参照

---

## 成功基準

1. safetensors形式のgpt-oss-20bモデルで推論が正常に動作する
2. safetensors形式のnemotronモデルで推論が正常に動作する
3. Metal/CUDA/ROCm/Vulkanの4プラットフォームで同一のテストがパスする
4. safetensors.cpp単体でビルドとテストが完結する（Router/xLLM依存なし）
5. ggmlをサブモジュールとして正しく管理できている
6. HuggingFace transformers（Python）と同等以上の推論速度を達成する
7. ストリーミング出力でトークンごとにコールバックが呼ばれる
8. continuous batchingで複数リクエストを効率的に処理できる
9. HuggingFace Tinyモデルを使用したCIテストがパスする

---

## 設計方針（インタビュー結果）

### safetensorsローダー
- stable-diffusion.cppの実装（model.cpp内のinit_from_safetensors_file）を参考に移植
- メモリマップドI/O（mmap）で高速ロードをサポート

### ggmlバージョン
- ggml本家リポジトリ（<https://github.com/ggml-org/ggml>）のmainに追従
- Attention実装はggml標準を使用（Flash Attentionは当面不要）

### KVキャッシュ
- llama.cppのKVキャッシュ実装を参考に実装
- KVキャッシュのINT8/FP8量子化をサポート

### サンプリング
- llama.cppのサンプリング実装を参考に実装

### エラーハンドリング
- llama.cppのコールバック方式を採用

### スレッドセーフティ
- 全てのAPIを完全スレッドセーフで設計

### C API
- プレフィックス: `stcpp_*`
- ビルドシステム: CMakeのみ

### テストモデル
- HuggingFaceのテスト用小型モデル（Tiny系）を利用

### ベンチマーク
- HuggingFace transformers（Python）との比較で性能を評価

### ドキュメント
- llama.cpp級の充実したドキュメントを目指す（APIリファレンス、チュートリアル、例示コード）

### Runtime統合
- 既存のエンジンプラグインアーキテクチャ（SPEC-d7feaa2c）に準拠

### リポジトリ
- 当面はnode/third_party/内で開発、将来的に個人アカウントで別リポジトリ化

### MVP定義
- 単一GPUでのgpt-oss-20b推論 + ストリーミング出力が動作すること

### LoRA/QLoRA
- LoRAアダプターのロードと推論時適用をサポート
- 動的ホットリロード（推論停止なしでアダプター切り替え）をサポート

### チャットテンプレート
- tokenizer_config.jsonのchat_template（Jinja2形式）を解析
- llama.cppのテンプレートエンジンを参考に実装

### マルチGPU
- Pipeline Parallelism（レイヤー単位でGPU分割）を採用
- Tensor Parallelismは将来検討

### プロンプトキャッシュ
- システムプロンプトやRAGコンテキストのKVキャッシュを再利用
- セッション間でキャッシュを共有可能

### プリフィル最適化
- プロンプト処理の並列化でTTFT（Time To First Token）を短縮

### Visionモデル
- LLaVA、Qwen-VL等の画像入力対応は将来実装予定
- 画像エンコーダーのプラグイン拡張を想定

# LLM Router

[English](./README.md) | 日本語

## 概要

LLM Router は、複数マシンに配置した C++ ノード（llama.cpp）を統合し、単一の OpenAI 互換 API
（`/v1/*`）を提供する Rust ルーターです。

## 主な特徴

- OpenAI 互換 API: `/v1/chat/completions`, `/v1/completions`, `/v1/embeddings`, `/v1/models`
- ロードバランシング: 利用可能なノードへ自動ルーティング
- ダッシュボード: `/dashboard` でノード、リクエスト履歴、ログ、モデルを管理
- ノード自己登録: ノードは起動時にルーターへ登録し、ハートビートを送信
- ノード主導モデル同期: ノードはルーターの `/v1/models` と `/v0/models/blob/:model_name` を参照して
  必要なモデルを取得（ルーターからの push 配布なし）
- クラウドプレフィックス: `openai:`, `google:`, `anthropic:` を `model` に付けて同一エンドポイントでプロキシ

## ダッシュボード

ルーターが `/dashboard` で提供します。

```
http://localhost:8080/dashboard
```

## LLM アシスタント向け MCP サーバー

LLM アシスタント（Claude Code など）は、専用の MCP サーバーを通じて LLM Router と連携できます。

### インストール

```bash
npm install -g @llm-router/mcp-server
# または
npx @llm-router/mcp-server
```

### 設定例 (.mcp.json)

```json
{
  "mcpServers": {
    "llm-router": {
      "type": "stdio",
      "command": "npx",
      "args": ["-y", "@llm-router/mcp-server"],
      "env": {
        "LLM_ROUTER_URL": "http://localhost:8080",
        "LLM_ROUTER_API_KEY": "sk_your_api_key"
      }
    }
  }
}
```

詳細は [mcp-server/README.md](./mcp-server/README.md) を参照してください。

## インストールと起動

### 前提条件
- Linux/macOS/Windows x64 (GPU推奨、GPUなしは登録不可)
- Rust toolchain (nightly不要) と cargo
- Docker (任意、コンテナ利用時)
- CUDAドライバ (GPU使用時。NVIDIAのみ)

### 1) Rustソースからビルド（推奨）
```bash
git clone https://github.com/akiojin/llm-router.git
cd llm-router
make quality-checks   # fmt/clippy/test/markdownlint 一式
cargo build -p llm-router --release
```
生成物: `target/release/llm-router`

### 2) Docker で起動
```bash
docker build -t llm-router:latest .
docker run --rm -p 8080:8080 --gpus all \
  -e OPENAI_API_KEY=... \
  llm-router:latest
```
GPUを使わない場合は `--gpus all` を外すか、`CUDA_VISIBLE_DEVICES=""` を設定。

### 3) C++ Node ビルド

```bash
npm run build:node

# 手動でビルドする場合:
cd node
cmake -B build -S .
cmake --build build --config Release
```

生成物: `node/build/llm-node`

### 4) 基本設定

#### ルーター（Rust）環境変数

| 環境変数 | デフォルト | 説明 |
|---------|-----------|------|
| `LLM_ROUTER_HOST` | `0.0.0.0` | バインドアドレス |
| `LLM_ROUTER_PORT` | `8080` | リッスンポート |
| `LLM_ROUTER_DATABASE_URL` | `sqlite:~/.llm-router/router.db` | データベースURL |
| `LLM_ROUTER_JWT_SECRET` | 自動生成 | JWT署名シークレット |
| `LLM_ROUTER_ADMIN_USERNAME` | `admin` | 初期管理者ユーザー名 |
| `LLM_ROUTER_ADMIN_PASSWORD` | - | 初期管理者パスワード |
| `LLM_ROUTER_LOG_LEVEL` | `info` | ログレベル |
| `LLM_ROUTER_HEALTH_CHECK_INTERVAL` | `30` | ヘルスチェック間隔（秒） |
| `LLM_ROUTER_NODE_TIMEOUT` | `60` | ノードタイムアウト（秒） |
| `LLM_ROUTER_LOAD_BALANCER_MODE` | `auto` | ロードバランサーモード |

クラウドAPI:

- `OPENAI_API_KEY`, `GOOGLE_API_KEY`, `ANTHROPIC_API_KEY`

#### ノード（C++）環境変数

| 環境変数 | デフォルト | 説明 |
|---------|-----------|------|
| `LLM_ROUTER_URL` | `http://127.0.0.1:11434` | ルーターURL |
| `LLM_NODE_PORT` | `11435` | HTTPサーバーポート |
| `LLM_NODE_MODELS_DIR` | `~/.runtime/models` | モデルディレクトリ |
| `LLM_NODE_BIND_ADDRESS` | `0.0.0.0` | バインドアドレス |
| `LLM_NODE_HEARTBEAT_SECS` | `10` | ハートビート間隔（秒） |
| `LLM_NODE_LOG_LEVEL` | `info` | ログレベル |
| `LLM_NODE_LOG_DIR` | `~/.llm-router/logs` | ログディレクトリ |

**注意**: 旧環境変数名（`ROUTER_HOST`, `LLM_MODELS_DIR`等）は非推奨です。
新しい環境変数名を使用してください。

### 5) 起動例
```bash
# ルーター
cargo run -p llm-router

# ノード (別シェル)
./node/build/llm-node
```

### 6) 動作確認
- ダッシュボード: `http://localhost:8080/dashboard`
- 健康チェック: `curl http://localhost:8080/v0/health`
- OpenAI互換: `curl http://localhost:8080/v1/models`

## 利用方法（OpenAI互換エンドポイント）

### 基本
- `POST /v1/chat/completions`
- `POST /v1/completions`
- `POST /v1/embeddings`

### クラウドモデルプレフィックス
- 付けるだけでクラウド経路に切替: `openai:`, `google:`, `anthropic:`（`ahtnorpic:` も許容）
- 例: `model: "openai:gpt-4o"` / `model: "google:gemini-1.5-pro"` / `model: "anthropic:claude-3-opus"`
- 転送時にプレフィックスは除去され、クラウドAPIへそのまま送られます。
- プレフィックスなしのモデルは従来どおりローカルLLMにルーティングされます。

### ストリーミング
- `stream: true` でクラウドSSE/チャンクをそのままパススルー。

### メトリクス
- `GET /v0/metrics/cloud` （Prometheus text）
  - `cloud_requests_total{provider,status}`
  - `cloud_request_latency_seconds{provider}`

## アーキテクチャ

LLM Router は、ローカルの llama.cpp ノードを調整し、オプションでモデルのプレフィックスを介してクラウド LLM プロバイダーにプロキシします。

### コンポーネント
- **Router (Rust)**: OpenAI 互換のトラフィックを受信し、パスを選択してリクエストをプロキシします。ダッシュボード、メトリクス、管理 API を公開します。
- **Local Nodes (C++ / llama.cpp)**: GGUF モデルを提供します。ルーターに登録し、ハートビートを送信します。
- **Cloud Proxy**: モデル名が `openai:`, `google:`, `anthropic:` で始まる場合、ルーターは対応するクラウド API に転送します。
- **Storage**: ルーターのメタデータ用の SQLite。モデルファイルは各ノードに存在します。
- **Observability**: Prometheus メトリクス、構造化ログ、ダッシュボード統計。

### システム構成

```
┌───────────────────────────────────────┐
│               Client                  │
│     (apps / users / integrations)     │
└───────────────────┬───────────────────┘
                    │ OpenAI-compatible (/v1/*)
                    ▼
┌───────────────────────────────────────┐
│                Router                 │
│  - /v1/* proxy + load balancing       │
│  - /v0/* (nodes, models, dashboard)   │
│  - /dashboard (Web UI)                │
└──────────────┬───────────────┬────────┘
               │               │
               │ OpenAI-compatible (/v1/*)
               ▼               ▼
        ┌────────────┐   ┌────────────┐
        │    Node    │   │    Node    │
        │ llama.cpp  │   │ llama.cpp  │
        └────────────┘   └────────────┘
               ▲               ▲
               │ /v0/nodes, /v0/health (registration/heartbeat)
               └───────────────┘
```

### リクエストフロー
```
Client
  │ POST /v1/chat/completions
  ▼
Router (OpenAI-compatible)
  ├─ Prefix? → Cloud API (OpenAI / Google / Anthropic)
  └─ No prefix → Scheduler → Local Node
                       └─ llama.cpp inference → Response
```

### モデル同期（push配布なし）

- ルーターは、登録・変換・キャッシュされたモデルだけを `/v1/models` に掲載します。
- ノードは `/v1/models` を参照してモデル一覧を取得します。
  - `path` が共有ストレージ等で参照可能なら、そのパスを直接使用します。
  - 参照できない場合は `/v0/models/blob/:model_name` からダウンロードしてローカルに保存します。
- ルーターからノードへの push 配布は行いません。

### スケジューリングとヘルスチェック
- ノードは `/v0/nodes` を介して登録します。ルーターはデフォルトで GPU のないノードを拒否します。
- ハートビートには、ロードバランシングに使用される CPU/GPU/メモリメトリクスが含まれます。
- ダッシュボードには `*_key_present` フラグが表示され、オペレーターはどのクラウドキーが設定されているかを確認できます。

## トラブルシューティング

### 起動時に GPU が見つからない
- 確認: `nvidia-smi` または `CUDA_VISIBLE_DEVICES`
- 環境変数で無効化: ノード側 `LLM_ALLOW_NO_GPU=true`（デフォルトは禁止）
- それでも失敗する場合は NVML ライブラリの有無を確認

### クラウドモデルが 401/400 を返す
- ルーター側で `OPENAI_API_KEY` / `GOOGLE_API_KEY` / `ANTHROPIC_API_KEY` が設定されているか確認
- ダッシュボード `/v0/dashboard/stats` の `*_key_present` が false なら未設定
- プレフィックスなしモデルはローカルにルーティングされるので、クラウドキーなしで利用したい場合はプレフィックスを付けない

### ポート競合で起動しない
- ルーター: `LLM_ROUTER_PORT` を変更（例: `LLM_ROUTER_PORT=18080`）
- ノード: `LLM_NODE_PORT` または `--port` で変更

### SQLite ファイル作成に失敗
- `LLM_ROUTER_DATABASE_URL` のパス先ディレクトリの書き込み権限を確認
- Windows の場合はパスにスペースが含まれていないか確認

### ダッシュボードが表示されない
- ブラウザキャッシュをクリア
- バンドル済み静的ファイルが壊れていないか `cargo clean` → `cargo run` を試す
- リバースプロキシ経由の場合は `/dashboard/*` の静的配信設定を確認

### OpenAI互換APIで 503 / モデル未登録
- 全ノードが `initializing` の場合 503 を返すことがあります。ノードのモデルロードを待つか、`/v0/dashboard/nodes` で状態を確認
- モデル指定がローカルに存在しない場合、ノードが自動プルするまで待機

### ログが多すぎる / 少なすぎる
- 環境変数 `LLM_ROUTER_LOG_LEVEL` または `RUST_LOG` で制御（例: `LLM_ROUTER_LOG_LEVEL=info` または `RUST_LOG=or_router=debug`）
- ノードのログは `spdlog` で出力。構造化ログは `tracing_subscriber` でJSON設定可

## モデル管理（Hugging Face, GGUF-first）

- オプション環境変数: レートリミット回避に `HF_TOKEN`、社内ミラー利用時は `HF_BASE_URL` を指定します。
- Web（推奨）:
  - ダッシュボード → **Models** → **Register**
  - Hugging Face repo（例: `TheBloke/Llama-2-7B-GGUF`）と、任意で filename を入力します。
  - `/v1/models` は、ルーターのファイルシステム上にキャッシュ済みのモデルだけを返します。

モデル ID はファイル名ベース形式に正規化されます（例: `llama-2-7b`, `gpt-oss-20b`）。

## API 仕様

### ルーター（Router）

#### OpenAI 互換（API キー認証）

- POST `/v1/chat/completions`
- POST `/v1/completions`
- POST `/v1/embeddings`
- GET `/v1/models`（API キーまたは `X-Node-Token`）
- GET `/v1/models/:model_id`（API キーまたは `X-Node-Token`）

#### ノード管理

- POST `/v0/nodes`（登録）
- GET `/v0/nodes`（一覧）
- DELETE `/v0/nodes/:node_id`
- POST `/v0/nodes/:node_id/disconnect`
- PUT `/v0/nodes/:node_id/settings`
- POST `/v0/health`（ノードからのヘルス/メトリクス送信、`X-Node-Token` 必須）
- GET `/v0/nodes/:node_id/logs`

#### モデル管理

- GET `/v0/models/available`（例: `?source=hf`）
- POST `/v0/models/register`
- GET `/v0/models/registered`
- DELETE `/v0/models/*model_name`
- POST `/v0/models/discover-gguf`
- POST `/v0/models/convert`
- GET `/v0/models/convert`
- GET `/v0/models/convert/:task_id`
- DELETE `/v0/models/convert/:task_id`
- GET `/v0/models/blob/:model_name`

#### ダッシュボード/監視

- GET `/v0/dashboard/overview`
- GET `/v0/dashboard/stats`
- GET `/v0/dashboard/nodes`
- GET `/v0/dashboard/metrics/:node_id`
- GET `/v0/dashboard/request-history`
- GET `/v0/dashboard/request-responses`
- GET `/v0/dashboard/request-responses/:id`
- GET `/v0/dashboard/request-responses/export`
- GET `/v0/dashboard/logs/router`
- GET `/v0/metrics/cloud`
- GET `/dashboard/*`
- GET `/playground/*`

### ノード（Node）

#### OpenAI 互換

- GET `/v1/models`
- POST `/v1/chat/completions`
- POST `/v1/completions`
- POST `/v1/embeddings`

#### 運用

- GET `/health`
- GET `/startup`
- GET `/metrics`
- GET `/metrics/prom`
- GET `/v0/logs?tail=200`
- GET `/log/level`
- POST `/log/level`

## 開発

```bash
make quality-checks
```

Dashboard を更新する場合:

```bash
pnpm install
pnpm --filter @llm-router/dashboard build
```

## ライセンス

MIT License

## 貢献

Issue、Pull Request をお待ちしています。

詳細な開発ガイドラインは [CLAUDE.md](./CLAUDE.md) を参照してください。

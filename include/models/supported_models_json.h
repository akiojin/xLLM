#pragma once

namespace xllm {

constexpr const char kSupportedModelsJson[] = R"json(
[
  {
    "id": "qwen2.5-7b-instruct",
    "name": "Qwen2.5 7B Instruct",
    "description": "Alibaba's multilingual instruction-tuned model with strong reasoning capabilities",
    "repo": "bartowski/Qwen2.5-7B-Instruct-GGUF",
    "recommended_filename": "Qwen2.5-7B-Instruct-Q4_K_M.gguf",
    "size_bytes": 4920000000,
    "required_memory_bytes": 7380000000,
    "tags": ["chat", "multilingual", "coding"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "7B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "qwen3",
    "name": "Qwen3 0.6B",
    "description": "Alibaba's compact Qwen3 model suited for lightweight inference",
    "repo": "bartowski/Qwen_Qwen3-0.6B-GGUF",
    "recommended_filename": "Qwen_Qwen3-0.6B-Q4_K_M.gguf",
    "size_bytes": 484220320,
    "required_memory_bytes": 726330480,
    "tags": ["chat", "multilingual", "compact"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "0.6B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "smollm2",
    "name": "SmolLM2 1.7B Instruct",
    "description": "Compact SmolLM2 instruct model for lightweight deployments",
    "repo": "bartowski/SmolLM2-1.7B-Instruct-GGUF",
    "recommended_filename": "SmolLM2-1.7B-Instruct-Q4_K_S.gguf",
    "size_bytes": 999117792,
    "required_memory_bytes": 1498676688,
    "tags": ["chat", "compact", "efficient"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_S",
    "parameter_count": "1.7B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "devstral-small",
    "name": "Devstral Small",
    "description": "Mistral AI's Devstral Small instruct model",
    "repo": "mistralai/Devstral-Small-2507_gguf",
    "recommended_filename": "Devstral-Small-2507-Q4_K_M.gguf",
    "size_bytes": 14333915904,
    "required_memory_bytes": 21500873856,
    "tags": ["chat", "efficient", "large"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "24B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "phi4",
    "name": "Phi-4",
    "description": "Microsoft's Phi-4 model for reasoning and coding tasks",
    "repo": "bartowski/phi-4-GGUF",
    "recommended_filename": "phi-4-Q4_K_M.gguf",
    "size_bytes": 9053114816,
    "required_memory_bytes": 13579672224,
    "tags": ["chat", "coding", "large"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "14B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "granite-4.0-micro",
    "name": "Granite 4.0 Micro",
    "description": "IBM Granite 4.0 Micro compact model",
    "repo": "ibm-granite/granite-4.0-micro-GGUF",
    "recommended_filename": "granite-4.0-micro-Q4_K_M.gguf",
    "size_bytes": 2099502528,
    "required_memory_bytes": 3149253792,
    "tags": ["chat", "compact", "efficient"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "3.5B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "granite-4.0-h-micro",
    "name": "Granite 4.0 H Micro",
    "description": "IBM Granite 4.0 H Micro compact model",
    "repo": "bartowski/ibm-granite_granite-4.0-h-micro-GGUF",
    "recommended_filename": "ibm-granite_granite-4.0-h-micro-Q4_K_M.gguf",
    "size_bytes": 1942564480,
    "required_memory_bytes": 2913846720,
    "tags": ["chat", "compact", "efficient"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "3.5B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "granite-4.0-h-tiny",
    "name": "Granite 4.0 H Tiny",
    "description": "IBM Granite 4.0 H Tiny compact model",
    "repo": "bartowski/ibm-granite_granite-4.0-h-tiny-GGUF",
    "recommended_filename": "ibm-granite_granite-4.0-h-tiny-Q4_K_M.gguf",
    "size_bytes": 4297134912,
    "required_memory_bytes": 6445702368,
    "tags": ["chat", "compact", "efficient"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "7B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "granite-4.0-h-small",
    "name": "Granite 4.0 H Small",
    "description": "IBM Granite 4.0 H Small model",
    "repo": "bartowski/ibm-granite_granite-4.0-h-small-GGUF",
    "recommended_filename": "ibm-granite_granite-4.0-h-small-Q4_K_M.gguf",
    "size_bytes": 19747023488,
    "required_memory_bytes": 29620535232,
    "tags": ["chat", "efficient", "large"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "24B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "mistral-nemo",
    "name": "Mistral Nemo Instruct 2407",
    "description": "Mistral Nemo Instruct 2407 model",
    "repo": "bartowski/Mistral-Nemo-Instruct-2407-GGUF",
    "recommended_filename": "Mistral-Nemo-Instruct-2407-Q4_K_M.gguf",
    "size_bytes": 7477208192,
    "required_memory_bytes": 11215812288,
    "tags": ["chat", "efficient", "large"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "12B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "deepseek-r1-distill-llama",
    "name": "DeepSeek R1 Distill Llama 8B",
    "description": "DeepSeek R1 Distill Llama 8B model",
    "repo": "bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF",
    "recommended_filename": "DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf",
    "size_bytes": 4920736608,
    "required_memory_bytes": 7381104912,
    "tags": ["chat", "efficient"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "8B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "magistral-small-3.2",
    "name": "Magistral Small 3.2",
    "description": "Magistral Small 3.2 instruct model",
    "repo": "bartowski/mistralai_Magistral-Small-2507-GGUF",
    "recommended_filename": "mistralai_Magistral-Small-2507-Q4_K_M.gguf",
    "size_bytes": 14333912032,
    "required_memory_bytes": 21500868048,
    "tags": ["chat", "efficient", "large"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "24B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "qwq",
    "name": "QwQ 32B",
    "description": "Qwen QwQ 32B model",
    "repo": "Qwen/QwQ-32B-GGUF",
    "recommended_filename": "qwq-32b-q4_k_m.gguf",
    "size_bytes": 19851335840,
    "required_memory_bytes": 29777003760,
    "tags": ["chat", "large"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "32B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "qwen3-coder",
    "name": "Qwen3 Coder 30B A3B Instruct",
    "description": "Qwen3 Coder 30B A3B instruct model",
    "repo": "unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF",
    "recommended_filename": "Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf",
    "size_bytes": 18556689568,
    "required_memory_bytes": 27835034352,
    "tags": ["chat", "coding", "large"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "30B A3B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "gemma3",
    "name": "Gemma 3 4B IT",
    "description": "Google Gemma 3 4B instruction-tuned model",
    "repo": "ggml-org/gemma-3-4b-it-GGUF",
    "recommended_filename": "gemma-3-4b-it-Q4_K_M.gguf",
    "size_bytes": 2489757856,
    "required_memory_bytes": 3734636784,
    "tags": ["chat", "compact"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "4B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "llama3.3",
    "name": "Llama 3.3 8B Instruct 128K",
    "description": "Llama 3.3 8B Instruct 128K model",
    "repo": "shb777/Llama-3.3-8B-Instruct-128K-GGUF",
    "recommended_filename": "llama-3.3-8b-instruct-q4_k_m.gguf",
    "size_bytes": 4920739296,
    "required_memory_bytes": 7381108944,
    "tags": ["chat", "efficient"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "8B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "llama3.1",
    "name": "Llama 3.1 8B Instruct",
    "description": "Llama 3.1 8B Instruct model",
    "repo": "unsloth/Llama-3.1-8B-Instruct-GGUF",
    "recommended_filename": "Llama-3.1-8B-Instruct-Q4_K_M.gguf",
    "size_bytes": 4920739200,
    "required_memory_bytes": 7381108800,
    "tags": ["chat", "efficient"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "8B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "llama3.2-3b-instruct",
    "name": "Llama 3.2 3B Instruct",
    "description": "Meta's lightweight instruction-tuned model optimized for edge deployment",
    "repo": "bartowski/Llama-3.2-3B-Instruct-GGUF",
    "recommended_filename": "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
    "size_bytes": 2020000000,
    "required_memory_bytes": 3030000000,
    "tags": ["chat", "lightweight"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "3B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "ministral3",
    "name": "Ministral 3",
    "description": "Mistral AI's compact instruct model for efficient deployment",
    "repo": "bartowski/mistralai_Ministral-3-3B-Instruct-2512-GGUF",
    "recommended_filename": "mistralai_Ministral-3-3B-Instruct-2512-Q4_K_M.gguf",
    "size_bytes": 2146498528,
    "required_memory_bytes": 3219747792,
    "tags": ["chat", "efficient", "compact"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "3B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "mistral-7b-instruct",
    "name": "Mistral 7B Instruct",
    "description": "Mistral AI's efficient instruction-tuned model with sliding window attention",
    "repo": "bartowski/Mistral-7B-Instruct-v0.3-GGUF",
    "recommended_filename": "Mistral-7B-Instruct-v0.3-Q4_K_M.gguf",
    "size_bytes": 4370000000,
    "required_memory_bytes": 6555000000,
    "tags": ["chat", "efficient"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "7B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "phi-3-mini",
    "name": "Phi-3 Mini",
    "description": "Microsoft's compact yet powerful model for reasoning and coding",
    "repo": "bartowski/Phi-3-mini-4k-instruct-GGUF",
    "recommended_filename": "Phi-3-mini-4k-instruct-Q4_K_M.gguf",
    "size_bytes": 2390000000,
    "required_memory_bytes": 3585000000,
    "tags": ["chat", "coding", "compact"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "3.8B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "gemma-2-9b",
    "name": "Gemma 2 9B",
    "description": "Google's open model with strong performance across diverse tasks",
    "repo": "bartowski/gemma-2-9b-it-GGUF",
    "recommended_filename": "gemma-2-9b-it-Q4_K_M.gguf",
    "size_bytes": 5760000000,
    "required_memory_bytes": 8640000000,
    "tags": ["chat", "multilingual"],
    "capabilities": ["TextGeneration"],
    "quantization": "Q4_K_M",
    "parameter_count": "9B",
    "format": "gguf",
    "engine": "llama_cpp",
    "platforms": ["macos-metal", "windows-directml", "linux-cuda"]
  },
  {
    "id": "gpt-oss-20b",
    "name": "GPT-OSS 20B",
    "description": "OpenAI's GPT-OSS 20B (Metal optimized)",
    "repo": "openai/gpt-oss-20b",
    "recommended_filename": "model.safetensors.index.json",
    "size_bytes": 13761316904,
    "required_memory_bytes": 20641975356,
    "tags": ["chat", "openai", "gpt-oss", "safetensors"],
    "capabilities": ["TextGeneration"],
    "quantization": null,
    "parameter_count": "20B",
    "format": "safetensors",
    "engine": "gptoss_cpp",
    "platforms": ["macos-metal"]
  },
  {
    "id": "gpt-oss-120b",
    "name": "GPT-OSS 120B",
    "description": "OpenAI's GPT-OSS 120B (Metal optimized)",
    "repo": "openai/gpt-oss-120b",
    "recommended_filename": "model.safetensors.index.json",
    "size_bytes": 65248893184,
    "required_memory_bytes": 97873339776,
    "tags": ["chat", "openai", "gpt-oss", "safetensors", "large"],
    "capabilities": ["TextGeneration"],
    "quantization": null,
    "parameter_count": "120B",
    "format": "safetensors",
    "engine": "gptoss_cpp",
    "platforms": ["macos-metal"]
  }
]
)json";

}  // namespace xllm
